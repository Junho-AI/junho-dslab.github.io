---
title:  "BERT 코드 리뷰1"
excerpt: "BERT"
toc: true
toc_sticky: true
categories:
  - NLP, BERT
tags:
  - NLP
  - BERT
---

## BERT?

![BERT](/assets/images/logo.png)
BERT란
* 1장에서는 패턴인식 문제를 해결과정에서 확률론이 얼마나 중요한지 리뷰 
* 2장에서는 몇몇 **확률 분포**의 예시와 성질에 대해서 공부할 예정  
* 또한 확률 분포를 바탕으로 베이지안 추론 등의 중요한 통계적인 개념들을 살펴볼 예정

2장에서 논의할 분포의 역할들 중 하나는 한정된 수의 관찰 집합 $x_{1},...,x_{N}$이 주어졌을 때 확률 변수 $x$의 확률 분포 $p(x)$를 모델링하는 것이다.

2장에서는 목표를 위해 **1) 데이터 포인터들은 독립적, 2) 동일하게 분포됨** 이라 가정
사실 밀도 추정 문제는 근본적으로 타당하지 않다고 한다. 제한된 수의 관찰된 데이터 집합으로부터 가능한 모 확률 분포의 가짓수는 무한이기 때문이다. 

*	모비율 : 지지율, 시청률, 실업률 따위와 같이 모집단에서 어떤 사건에 대한 비율
*	그러면 어떻게 해야되는거지..?

각각의 데이터 포인트 $x_{1},...,x_{N}$에 대해서 0이 아닌 값을 가지는 어떤 분포 $p(x)$도 모 분포의 후보가 될 수 있다. 이들 중에서 적절한 분포를 선택하는 것은 1장의 다항식 곡선 근사 문제에서 맞닥뜨렸던 모델 선택의 문제와 연관되어 있다. 이는 패턴 인식 문제의 중요 쟁점 사항 중 하나라고 한다.

*	결국 적절한 분포를 택하는게 좋다?

따라서 2장에서는 우선 **이산 확률 변수의 이항분포와 다항분포**에 대해 살펴보고, 그 다음엔 **연속 확률 변수의 가우시안 분포**에 대해서 알아본다고 한다. 이 분포들은 **매개변수적** 분포의 예시이다. 이렇게 불리는 이유는 분포들이 작은 수의 조절 가능한 매개변수에 의해 결정되기 때문이라고 한다. 이런 매개변수의 예시로 가우시안 분포의 평균와 분산이 있다고 한다. 

이러한 모델을 밀도 추정 문제에 적용하기 위해서는 관찰된 데이터 집합을 바탕으로 적절한 매개변수의 값을 구하는 과정이 요구된다. 빈도적 관점에서는 어떤 특정 기준을 최적화 하는 방식으로 매개변수를 찾게 된다. 그 예시로는 가능도 함수(Likelihood)가 있다. 이와는 반대로 베이지안 관점에서는 매개변수에 대한 사전 분포를 바탕으로 관측된 데이터 집합이 주어졌을 때의 해당 사후 분포를 계산한다(베이지안 정리를 사용하여).

*	특정 기준을 최소화하는 방식으로 매개변수를 찾기(빈도적 관점)
*	매개변수에 대한 사전 분포 + 관측된 데이터 집합 -> 사후 분포 측정 (베이지안)

**켤레**(_conjugate_) 사전 확률이 중요한 역할을 한다는 것도 살펴본다고 한다. 켤레 사전 확률은 사후 확률이 사전 확률과 같은 함수적 형태를 띄도록 만들어 준다.

*	사전분포와 사후분포가 모숫값만 다르고 함수 형태가 같은 확률밀도함수로 표현될 수 있도록 해주는 사전분포를 켤레 사전확률분포라고 한다.
*	결국 사전분포와 사후분포가 비슷하게 생겼다는 뜻

그 결과, 베이지안 분석이 매우 단순해진다. 예를 들어, 다항 분포 매개변수의 켤레 사전 확률은 **디리클레 분포**(_Dirichlet distribution_)이며, 가우시안 분포 평균값의 켤레 사전확률은 또 다른 가우시안 분포이다. 이 분포들은 모두 **지수족**(_exponential family_)에 속한다. 지수족 분포들은 몇몇 중요한 성질들을 가지고 있는데, 이에 대해 추후에 설명한다고 한다.

*	디리클레 분포는 연속 확률분포의 하나로, $k$차원의 실수 벡터 중 벡터의 요소가 양수이며 모든 요소를 더한 값이 1인 경우($k-1$차원 단체(_simplex_)) 에 대해 확률값이 정의되는 분포이다. (_위키백과_)
*	지수족 : 지수함수와 연관되어 있는 특정 확률분포 종류를 가리키는 말로, 정규 분포나 감마 분포, 다항 분포 등 일반적으로 널리 사용되는 분포들이 다수 포함되어 있다.

매개변수적인 접근법의 한계점 중 하나는 분포가 특정한 함수의 형태를 띠고 있다고 가정한다는 것. 이는 몇몇 적용 사례의 경우에는 적절하지 않다. 이럴 땐 **비매개변수적**(_nonparametric_) 밀도 추정 방식이 대안으로 활용될 수 있다고 한다. 비매개변수적 밀도 추정 방식에서는 분포의 형태가 데이터 집합의 크기에 종속적이다. 이러한 모델들은 매개변수를 가지고 있지만, 분포 형태를 결정짓는 것이 아니라 모델 복잡도에 영향을 미친다고 한다. 2장 마지막에서는 히스토그램, 최근점 이웃, 커널을 바탕으로 한 비매개변수적 방법에 대해서 논의할 것이다.

## 2.1 이산 확률 분포

하나의 이진 확률 변수 $x\in\{0,1\}$을 고려해 본다. 예를 들어 $x$는 동전 던지기의 결괏값을 설명하는 확률 변수이다. $x = 0,1$은 동전의 앞,뒤인 경우로 판단하면 될 듯 하다. 만약 동전이 망가져서 앞면이 나올 확률과 뒷면이 나올 확률이 동일하지 않다고 가정해보자. 이때 $x=1$일 확률은 매개변수 $\mu$를 통해 다음과 같이 표현할 수 있다.

### (식 2.1)<center>$p(x = 1\mid\mu) = \mu$</center>

여기서 $0 \leqslant \mu \leqslant 1$이다. 그리고 $p(x = 0\mid\mu) = 1 - \mu$가 된다. 따라서 $x$에 대한 확률 분포를 다음과 같이 적을 수 있다.

### (식 2.2)<center>$Bern(x\mid\mu) = \mu^x(1 - \mu)^{1-x}$</center>

이것을 **베르누이 분포(_Bernoulli distribution_)**라고 한다. 베르누이 분포는 정규화되어 있으며, 그 평균과 분산이 다음과 같이 주어진다는 것을 쉽게 증명할 수 있다.

### (식 2.3)<center>$\mathbb{E}[x] = \mu$</center>
### (식 2.4)<center>$var[x] = \mu(1-\mu)$</center>

증명되는 과정은 다음과 같다.

* $\mathbb{E}[x] = 1\cdot\mu+0\cdot(1-p) = p$
* $var[x] = E(X-p)^2$
* $		  = (0-p)^2 \cdot (1-p) + (1-p)^2 \cdot p$
* $		  = p(1-p)$

$x$의 관측값 데이터 집합 $\mathcal{D} = \{x_{1},...,x_{N}\}$이 주어졌다고 가정하면, 관측값들이 $p(x\mid\mu)$에서 독립적으로 추출되었다는 가정하에 $\mu$의 함수로써 가능도 함수를 구성할 수 있다.

### (식 2.5)<center>$p(\mathcal{D}\mid\mu) = \prod_{n=1}^N p(x_{n}\mid\mu) = \prod_{n=1}^N \mu^x_{n}(1-\mu)^{1-x_{n}}$</center>		

빈도적 관점에서는 가능도 함수를 최대화하는(또는 이와 동일하게 로그 가능도 함수를 최대화하는) $\mu$를 찾아서 $\mu$의 값을 추정할 수 있다. 베르누이 분포의 경우 로그 가능도 함수는 다음으로 주어진다.

### (식 2.6)<center>$\ln p(\mathcal{D}\mid\mu) = \sum_{n=1}^N \ln p(x_{n}\mid\mu)$</center> 
### (식 2.6)<center>$\ln p(\mathcal{D}\mid\mu)= \sum_{n=1}^N {x_{n}\ln\mu + (1-x_{n})\ln(1-\mu)}$</center>

*	베르누이 분포의 경우 로그를 취하여 로그 가능도 함수를 구할 수 있으며, $\sum_{n}x_{n}$을 통해서만 $N$개의 관측값 $x_{n}$과 연관된다. 이는 **충분 통계량(_sufficient statistic_)**의 예시 중 하나이다.
*	$\sum_{n=1}^N$이 의미하는 것이 관측값의 합이고 이는 충분 통계량이라고 해석하였다.

여기에 $\ln p(\mathcal{D}\mid\mu)$를 $\mu$에 미분하고 이를 0과 같다고 가정하면 다음과 같은 최대 가능도 추정값을 구할 수 있다.

### (식 2.7)<center>$\mu_{ML} = {1 \over N}\sum_{n=1}^N x_{n}$</center>

식 2.7은 **표본 평균(_sample mean_)**이라고 불리며, $x=1$인 관찰값의 수를 $m$이라고 한다면 식 2.7를 다음과 같이 적을 수 있다

### (식 2.8)<center>$\mu_{ML} = {m \over N}$</center>
*	여기서 $m$은 나온 개수, $N$은 전체라고 보면 될 것 같다.

즉, 최대 가능도 체계하에서는 데이터 집합을 토대로 앞으로 동전이 나올 확률을 구하는 것이다. 이러한 방식은 데이터의 수가 적고, 값이 편향되어 있으면 문제가 생긴다. 데이터셋의 크기가 3인데 3번 모두 앞면이 나온다면, 앞으로 나올 동전은 전부 앞면이 되는 것이다. $\mu_{ML} = {3 \over 3} = 1$ 이는 과적합의 극단적인 사례이다.

만약 크기 $N$의 데이터가 주어지고 $x=1$인 관측값의 수 $m$에 대한 분포를 생각해볼 수 있으며, 이를 **이항 분포(_binomial distribution_)**이라고 한다.
*	이항분포 : 연속 독립된 $n$번 시행에서 각 시행의 확률이 $p$를 가질때의 이산확률분포
*	성공률이 $p$, 실패율이 $1=p$이고, $n$번 시행이 나타나는 분포 -> 이항분포

이항분포는 식 2.5와 비례한다는 것을 알 수 있고, 정규화 개수를 구하기 위해 모든 동전 던진 횟수 $N$에 대해 나올 수 있는 앞면의 수 $m$의 모든 가짓수를 구해야 한다. 따라서 이항 분포는 다음과 같다

### (식 2.9)<center>$Bin(m \mid N,\mu)={N \choose m}\mu^m (1-\mu)^{N-m}$</center>
### (식 2.10)<center>${N \choose m} \equiv {N! \over (N-m)!m!}$</center>
*	식 2.10은 $N$번 던져서 나올 수 있는 $m$의 모든 가짓수이다

이항분포의 평균과 분산은 다음과 같이 구할 수 있다. 위에 언급하였듯이 2장에서의 데이터 포인터들은 독립적이고 동일하게 분포되어 있다. 연습문제 1.10에서 사건들의 합의 평균값은 평균값들의 합과 같으며, 사건들의 합의 분산은 분산들의 합과 같다는 것을 증명하였다.

따라서 $m = x_{1}+\cdots+x_{N}$이기 때문에 관측값에 대해서 평균과 분산은 다음과 같다.

### 식(2.12.1)<center>$\mathbb{E}[m] \equiv \sum_{m=0}^N mBin(m \mid N, \mu) = N\mu$</center>
### 식(2.12.1)<center>$var[m] \equiv \sum_{m=0}^N (m-\mathbb{E}[m])^2 Bin(m \mid N, \mu) = N\mu(1-\mu)$</center>

## 2.1.1 베타분포
식 2.8인 $\mu_{ML} = {m \over N}$는 과적합이 일어나기 쉽다고 언급하였다. 이 문제를 베이지안적으로 접근하기 위해서는 매개변수 $\mu$에 대한 사전분포 $p(\mu)$를 도입하는 것이 필요하다.

* 	가능도 함수 : $p(\mathcal{D}\mid\mu) = \prod_{n=1}^N \mu^x_{n}(1-\mu)^{1-x_{n}}$ 

가능도 함수가 $\mu^x(1-\mu)^{1-x}$의 형태를 가지는 인자들의 곱의 형태를 띄고 있다. 만약 $\mu$와 $(1-\mu)$의 거듭제곱에 비례하는 형태를 사전분포로 택한다면, 사전 확률과 가능도 함수의 곱에 비례하는 사후분포 역시 같은 함수적 형태를 가질 것이다. 이러한 성질을 **켤레성(_conjugacy_)**이라고 한다.

*	따라서 사전분포로 **베타분포(_beta distribution_)**을 사용할 것이다.

### (식 2.13) <center>$Beta(\mu\mid a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}$</center>

*	여기서 $\Gamma(x)$는 식 1.141에서 정의된 감마함수라고 한다.
*	$\Gamma(x) \equiv \int_{0}^{\infty} \mu^{x-1} e^{-\mu}, du$

식2.13의 계수들은 베타분포가 정규화되도록 한다.

### (식 2.14)<center>$\int_{0}^{1} Beta(\mu \mid a,b) du = 1$</center>

따라서 베타 분포의 평균과 분산은 아래와 같이 주어진다고 한다.

### 식(2.15)<center>$\mathbb{E}[\mu] = \frac{a}{a+b}$</center>
### 식(2.16)<center>$var[\mu] = \frac{ab}{(a+b)^2(a+b+1)}$</center>

$a,b$는 $\mu$의 분포를 결정하기에 **초매개면수(_hyperparameter_)**라고 한다. 그림2.2에서 다양한 초개매변숫값에 따른 베타분포의 도표를 확인할 수 있다.	

오른쪽의 식2.13(베타사전분포)과 식2.9(이항가능도)를 곱한 후 정규화를 구함으로써 $\mu$의 사후분포를 구할 수 있다. $\mu$와 관련된 인자만 남기면 사후분포가 아래와 같은 형태를 가지는 것을 확인할 수 있다.

### 식(2.17)<center>$p(\mu \mid m,l,a,b) \propto \mu^{m + a - 1} (1-\mu)^{l+b-1}$</center>
*	$l = N - m$ -> 뒷면의 개수
*	사후분포가 사전분포인 식2.13과 비슷하다 or $\mu$에 대해서 같은 함수적 종속성을 가지고 있다. -> 켤레적인 성질

이 새로운 베타분포의 정규화 계수는 식 2.13과의 비교를 통해 구할 수 있다.

### 식(2.18)<center>$p(\mu \mid m,l,a,b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)} \mu^{m+a-1} (1-\mu)^{l+b-1}$</center>
*	식 2.10를 참조하자

$x=1$인 값이 $m$개 있고 $x=0$인 값 하나가 있는 데이터 집합을 관찰한 결과, 사전분포와 비교했을 때, 사후분포에서는 $a$의 값이 $m$만큼, $b$의 값이 $l$만큼 증가하였음을 확인할 수 있다.

이 사실로부터 사전분포의 초매개변수인 $a, b$를 각각 $x=1, x=0$인 경우에 대한 **유효 관찰수(_effective number of observations_)**로 해석할 수 있다.

결국엔 우리가 추가적으로 관측 데이터를 얻게 된다면, 지금의 사후분포가 새로운 사전분포가 될 수 있음을 암시한다. 이를 확인하기 위해 관측값을 한 번에 하나만 받아들이고, 현재의 사후분포를 매번 업데이트한다 치자. 

*	베이지안 정리
*	새로 수정된 사후 분포 $\propto$ 가능도 $\times$ 새로운 관측 값 $+$ 정규화

각 단계에서 사후분포는 $x=1$과 $x=0$에 해당하는 전체 관측값들의 숫자가 새로운 $a,b$로 주어지는 사후분포에 해당한다. 즉 $x=1$인 새로운 관측값들이 주어지면 단순히 $a$를 1 증가시키고 0이면 $b$를 1 증가하면 된다.

그림 2.3를 보면 새로운 관측값 $x=1$로 인해 가운데 가능도 함수가 $(0,0), (0,1)$로 이루어졌고, 그로인해 사후분포가 오른쪽으로 치중된 모습을 볼 수 있다. 더나아가 그림2.2를 보면, 새로운 관측값의 수인 $a,b$가 증가함에 따라 사후분포의 불확실성이 줄어들게 된다.

*	이는 데이터가 많아질수록 과적합이 해결되는 당연한 것이 아닌가?
* 	결국 베이지 관점을 받아들이면 학습에 있어서 **순차적(_sequential_)**인 접근이 가능해지며, 큰 데이터 집합을 처리하는데도 적합하고 최대 가능도 방법론 역시 순차적 방법론하에서 사용가능하다.

if 우리의 목표가 다음 시도의 결과값 예측 잘하기라면, 관측 데이터 집합 $\mathcal{D}$가 주어진 상황하에서의 $x$의 예측분포를 계산해야 한다. 확률의 합과 곱 법칙에 따라 다음의 형태를 띤다.

### (식 2.19)<center>$p(x=1\mid\mathcal{D}) = \int_{0}^{1} p(x=1\mid\mu)p(\mu\mid\mathcal{D}) du$ </center>

### (식 2.19)<center>$p(x=1\mid\mathcal{D}) = \int_{0}^{1} \mu p(\mu\mid\mathcal{D}) du = \mathbb{E}[\mu\mid\mathcal{D}]$</center>

사후분포 $p(\mu\mid\mathcal{D})$에 대한 결과인 식2.18과 베타분포의 평균에 해당하는 식2.15를 이용하여 아래를 구할 수 있다.

### (식 2.20)<center>$p(x=1\mid\mathcal{D}) = \frac{m+a}{m+a+l+b}$</center>

식 2.20은 전체 관측값에서 $x=1$인 관측값 비율이며, $m,l \to \infty$라면 식2.20은 식2.8의 최대 공분산 결과값과 같아진다. 베이지안의 결과값과 최대 공분산의 결과값이 무한하게 큰 데이터집합에서 같아지는건 매우 일반적이다.

만약 제한된 크기의 데이터 집합에서 $\mu$의 사후 평균값은 사전 평균값과 식2.7에서 주어진 사건의 상대적인 빈도수를 바탕으로 한 최대 가능도 추정치에 있게 된다.

##2.2 다항변수

*	이산 확률 변수 : 2가지 중 하나를 취하는 수량을 설명하는데 사용
*	하지만 서로 다른 $K$개의 값들 중 하나를 취할 수 있는 이산변수는?
*	**One Hot Encoding**

원 핫 인코딩에서는 각각의 변수가 K차원의 벡터 x로 나타내지며, $x_{k}$ 값들 중 하나는 1, 나머지는 0으로 설정된다.

### (식 2.25)<center>$x = (0, 0, 1, 0, 0, 0)^{T}$</center>
*	$\sum_{k=1}^K x_{k} = 1$을 만족

### (식 2.26)<center>$p(x\mid\mu) = \prod_{k=1}^K \mu_{k}^{x_{k}}$</center>
*	$x_{k} = 1$이 될 확률을 $\mu_{k}$라고 한다면 x의 분포
*	식2.2 $Bern(x\mid\mu) = \mu^x(1 - \mu)^{1-x}$

식2.26은 베르누이 분포이지만, 결과값이 2가지 이상인 경우로 일반화한 것이라 볼 수 있다. 따라서 식2.27, 2.28처럼 쉽게 증명할 수 있다.

만약 관측값 하나인 $x$에서 N개의 독립적인 관측값들의 집합인 $mathcal{D}$인 경우에는 다음과 같다.

### (식 2.29)<center>$p(\mathcal{D}\mid\mu) = \prod_{k=1}^K \mu_{k}^{m_{k}}$</center>
*	$ x \to m$

### (식 2.30)<center>$m_{k} = \sum_{n} x_{nk}$</center>
*	가능도 함수값이 $K$값을 통해서만 $N$개의 데이터 포인트와 연관되어 있음을 확인
*	식2.30은 $x_{k} = 1$인 관측값의 숫자에 해당. 이를 **충분통계량**이라고 한다.

*	베르누이 분포+ 이산확률분의 충분통계량 : $\ln p(\mathcal{D}\mid\mu)= \sum_{n=1}^N {x_{n}\ln\mu + (1-x_{n})\ln(1-\mu)}$

이전 식2.5에서~식2.7에서 볼 수 있다싶이, 최대 가능도 해를 찾을 수 있으며 다음과 같다.

### (식 2.33)<center>$\mu_{k}^{ML} = \frac{m_{k}}{N}$</center>
*	식 2.8 : $\mu_{ML} = {m \over N}$

결국 매개변수 $\mu$와 관측값의 숫자 $N$에 의해 결정되는 수량 $m_{1},\cdots,m_{k}$의 결합분포를 고려해보자.


### (식 2.34)<center>$Mult(m_{1},m_{2},\cdots,m_{k}\mid\mu,N) = \begin{pmatrix} N \\ m_{1},m_{2}\cdots m_{K} \end{pmatrix} \prod_{k=1}^K \mu_{k}^{m_{k}}$</center>
*	이 식을 다항분포라고 한다.
*	결국  2.9와의 차이는 관측값이 몇개냐의 차이일뿐, 표현하는 방식은 같게 이루어질 수 있다, 원핫인코딩을 사용해서

식 2.34와 똑같이 $\mu$에 대해 사전분포를 확인해보자
### (식 2.37)<center>$p(\mu\mid\alpha) \propto \prod_{k=1}^K \mu_{k}^{a_{k}-1}$</center>
*	식2.7 : $p(\mu \mid m,l,a,b) \propto \mu^{m + a - 1} (1-\mu)^{l+b-1}$
*	= 이항분포

이 분포의 정규화된 형태는 다음과 같다.
### (식 2.38)<center>$Dir(\mu\mid\alpha) = \frac{\Gamma(\alpha_{0}}{\Gamma(\alpha_{1})\cdots\Gamma(\alpha_{K})}$</center>
*	$\alpha_{0} = \sum_{k=1}^K \alpha_{k}$
*	이 식은 **디리클레 분포(_Dirichlet distribution_)** 라고 한다.
*	식2.13 : $Beta(\mu\mid a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}$

베이지안 정리($사후분포 \propto 가능도 \times 사전확률$)에 따라 사후분포는 다음과 같은 형태를 보인다. 
### (식 2.40)<center>$p(\mu\mid\mathcal{D},\alpha) \propto p(\mathcal{D}\mid\mu) \propto \prod_{k=1}^K \mu_{k}^{a_{k}+m_{k}-1}$</center>

이를 통해 디리클레 분포는 다항 분포의 켤레 사전분포임을 확인할 수 있다. 여기에 정규화 계수를 구한다면 다음과 같다.
### (식 2.41)<center>$p(\mu\mid\mathcal{D},\alpha) = Dir(\mu\mid\alpha+m)$</center>
### (식 2.41)<center>$p(\mu\mid\mathcal{D},\alpha) = \frac{\Gamma(\alpha_{0}+N)}{\Gamma(\alpha_{1}+m_{1})\cdots\Gamma(\alpha_{k}+m_{k})} \prod_{k=1}^K \mu_{k}^{a_{k}+m_{k}-1}$</center>
*	식2.18 : $p(\mu \mid m,l,a,b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)} \mu^{m+a-1} (1-\mu)^{l+b-1}$

### (식 2.25)<center>$$</center>
### (식 2.25)<center>$$</center>
### (식 2.25)<center>$$</center>

























