I"Xr<h2 id="abstract">Abstract</h2>
<p>데이터 증강은 적은 리소스 환경에서의 과적합 문제와 클래스 불균형 문제를 해결하기 위해 풍부한 학습 데이터 샘플 구축을 목표로 한다. 기존의 방법은 동의어 대체와 같은 분류하고자 하는 테스트에 맞게 최적화된 기법을 고안한 다음, 높은 성능을 높이는 하이퍼 파라미터를 인위적으로 설정하는 방식으로 이루어졌다. 하지만 이러한 방식은 많은 사전 지식이 필요하고, 차선책에 그칠 가능성이 있다. 게다가 증강 기법의 파라미터 수는 이전 연구들에 의해 제한적으로 정해질 수 있으며, 증강 데이터의 품질(다양성)이 떨어질 수 있고 성능 향상에 제약이 있을 수 있다. 이러한 문제를 해결하기 위해, 저자는 Text AutoAugment(TAA)라는 프레임워크를 제안한다. TAA는 데이터 증강을 위한 compositional하고 learnable한 패러다임이다. 이전 논문과의 차이점은 증강 정책(policy)에서의 다양한 기법들(operations)의 결합으로 간주하고, Bayesian Optimization 알고리즘을 활용한다. 또한 모델이 최적의 증강 파라미터를 찾도록 유도함으로써 모델의 일반화(generalization) 능력을 크게 향상시킨다. 저자의 방식은 데이터의 수가 부족하고, 클래스 데이터의 불균형이 있는 6개의 벤치마크 분류 성능에서 정확도를 평균 8.8%, 9.7% 향상시켰다.</p>

<h2 id="1-introduction">1. Introduction</h2>
<p>텍스트 분류와 같은 자연어 처리 테스트에서의 모델 성능은 대부분 양질의 학습 데이터에 크게 의존한다. 그러나 이는 데이터를 구축함에 있어서 시간소모적(time-consuming)이며 노동집약적(labor-intensive)이며, 대부분은 적은 데이터 환경에서 진행될 수 밖에 없을 것이다. 데이터 증강은 분류 성능을 높이기 위한 row instances를 기반으로 하여, label를 변환하지 않는 선에서 추가적인 instance를 합성하는 방식으로 이루어진다. 이전 논문에서의 데이터 증강은 데이터가 불충분하고 불균형 클래스와 같은 다양한 시나리오에서의 우수함을 입증하는 방식으로 접근한다.</p>

<p>이전 증강 방식은 크게 “생성”과 “수정”의 2가지로 나눌 수 있다. 생성 방식의 증강 방법들은 원본 문장에서 paraphrases를 통합하기 위해 조건부(conditional) 생성 모델을 사용한다 <strong>(원본 데이터셋의 표현 방식들을 최대한 유지하겠다라는 뜻으로 해석하였음)</strong>. 이러한 방식은 문장의 유창함이나 라벨 보존성에서 장점을 가지고 있지만, GPT와 같은 대용량 모델을 fine-tuning하기에 부담감이 있다. 대신에 수정 방식의 증강 방법들은 삭제나 swap과 같은 기법들을 증강이 필요한 label(데이터 수가 적은)의 문장에 적용하는 방식이다. 이 방식은 거대 언어 모델을 학습 시킬 필요가 없이, 간단하고 효율적이다. 그러나 수정 방식의 단점은 얼마나 문장 내 단어를 바꿀 것인지 정하는 파라미터에 대해 예민하다 <strong>(EDA의 경우, 0.0~1.0에서 각자 학습 데이터에 맞는 적절한 수를 찾기에는 어렵다. 물론 추천하는 파라미터는 존재하지만).</strong></p>

<p><img src="/assets/img/TAA_figure1.png" alt="" width="500" height="500" class="align-center" /></p>

<p>Figure 1를 보면, IMDB 데이터셋에 있어서 수정될 단어의 수를 정하는 여러 파라미터에 대해 분류 성능을 조사하였다. TF-IDF 대체 모델의 경우 0.4이하로 성능이 떨어짐을 확인하였다. 논문의 저자는 heuristic한 하이퍼 파라미터의 설정은 효율성이 떨어지고 차선책에 빠지기 쉽다고 언급하고 있다 <strong>(내 생각으로는 임의적으로 본인이 “0.6 이상이면 문장 내 다양한 단어로 많이 바뀔테니깐 성능이 높아지겠지?”라고 설정하게 되는 순간, 오히려 성능 하락으로 이어질 가능성이 크다고 주장하는 것 같다).</strong> 게다가 다른 수정 방식의 논문들에서는 한 문장에 단 한번의 증강 기법을 적용하고 이것이 증강 문장 표현의 다양성을 하락시키며, 성능을 하락시킨다. <strong>(어느 한 문장은 일부 단어 몇개만 삭제해야 성능이 오르고, 다른 문장은 대체만 해야 성능이 오른다고 가정한다면, 모든 문장에 동일한 증강 기법(RS,RI,RD,SR과 같은)을 사용하면 안된다는 것으로 해석)</strong></p>

<p>이러한 문제를 극복하기 위해, 논문의 저자는 데이터 증강에 있어서 compositional하고 learnable한 패러다임인 Text AutoAugment(TAA) 프레임워크를 제안하였다. 저자의 목표는 분류 모델의 성능을 증가시키기 위해, 수정 방식 증강 방법에서 학습 데이터에 맞게 최선의 하이퍼 파라미터를 자동적으로 정하도록 하는 것이다. 저자는 증강 방식에서 사용될 policy 목록을 \(\{op_{1}, \cdots , op_{N}\}\) 와 같이 설정하였으며, \({op}\) 내부에는 \({type \;t,\;probability \;p,\;magnitude \; \lambda}\)의 3가지 파라미터가 존재한다. 다음과 같은 구도는 원본 문장에 대해 하나 이상의 operation을 적용할 수 있으며, systhetic instances의 다양성을 높일 수 있다. <strong>(policy안에 여러개의 \(op\)가 있기 때문에.)</strong> 최종적으로 policy solution은 operation set과 각 operation에서의 수정 파라미터의 2가지 형태의 지식을 학습하게 된다. 저자는 최적의 policy를 찾기 위해, 새로운 objective function를 제안하고, Sequential Model-based Global Optimization (SMBO)를 사용했으며, 효율적이고 광범위하게 쓰이는 메소드인 AutoML를 최적의 policy를 학습하는데 사용하였다.</p>

<p>저자의 알고리즘은 주어진 타겟 데이터셋에 적응(adaptively)하면서 자동적으로 최적의 증강 policy를 찾는다고 한다. 또한 distributed된 learning 프레임워크는 몇몇의 GPU 환경에서 기대할만한 결과를 얻을 수 있다고 한다. 요약하자면, 논문이 공헌할 수 있는 바는 크게 2개로 요약할 수 있다.</p>

<ul>
  <li>우리는 데이터 증강을 위한 learnable and compositional framework을 제안한다. 우리의 목적은 알고리즘이 자동적으로 최적의 compositional policy를 찾으며, 해당 policy는 증강된 데이터의 양과 질, 다양성을 향상시킨다.</li>
  <li>적은 데이터 환경, 클래스 불균형 데이터 형태를 띄는 6개의 벤치마크 데이터셋에서, TAA는 BERT와 같은 Neural networks에서 generalization 성능을 상당히 많이 높일 수 있으며, 분류 성능 또한 효과적인 향상을 보였다.</li>
</ul>

<h2 id="2-text-autoaugment">2. Text AutoAugment</h2>
<p>이번 섹션에서는, TAA가 어떻게 최적의 증강 policy를 찾도록 동작하는지 소개한다. 첫번째로 TAA의 policy는 다양한 operation들의 조합으로 구성되어 있으며, 계층적인 구조를 띄고 있다. 자세한 설명은 Section 2.1에 나와있다. 두번째는 전체적인 개요와 global objective function에 대한 설명이며 Section 2.2에 있다. 마지막으로는 구체적인 policy optimization에 대해서 설명한다. 위에서 언급하였듯이 하나의 policy는 \({N}\)개의 다양한 operation으로 구성되어 있다.</p>

<h2 id="21-compositional-augmentation-policy">2.1 Compositional Augmentation Policy</h2>
<p><img src="/assets/img/TAA_figure2.png" alt="" width="1000" height="900" class="align-center" /></p>

<p>다양한 표현을 가진 증강 문장을 생성하기 위해, 저자는 하나의 operation 대신에 compositional policy를 소개한다. 
Figure2는 해당 policy \({\mathcal P}\)의 사용법에 대해서 보여준다.</p>

<div class="align-center">\[{\mathcal P = {\{\mathcal O_{1}, \cdots ,\mathcal O_{i}, \cdots , \mathcal O_{N}\}}}\]
</div>

<p>operation \({\mathcal O}\)는 atomic 구성 요소이며, 텍스트 \({x}\)를 증강된 문장 \({x_{aug}}\)로 만들어준다. 각 operation \({\mathcal O_{i}}\)는 위에서 또한 언급하였듯이 3개의 요소로 구성되어 있다.</p>

\[{\mathcal O_{i} = \left\langle t_{i},p_{i},{\lambda_{i}} \right\rangle}\]

<p>3개의 파라미터에서 
(1) Type \({t}\) 은 EDA와 유사하게 Random Swap, Random Delete, TF-IDF Insert, TF-IDF Substitute, WordNet Substitute 로 구성되어 있다.
(2) Probability \({p}\) 는 0~1 사이에서의 확률을 의미한다. 이는 증강 기법을 적용할 확률 값으로, 예를 들어 0.6이라고 하면 증강 되지 않을(원본 문장) 확률은 0.4이다.
(3) Magnitude \({\lambda}\) 는 변환된 단어의 비율을 결정한다. 값은 0~0.5 사이의 값이다.
이를 정리하여, 증강될 문장 기법을 공식화 하면 아래와 같다.</p>

<p><img src="/assets/img/TAA_eqution1.png" alt="" width="500" height="500" class="align-center" /></p>

<p>이는 위에서 설명하였듯이 \({p}\) 에 따라서 증강이 진행되냐 아니냐를 결정한다. <strong>(이 또한 자동으로 설정되는지는 확인해봐야 함)</strong>. 증강되는 문장의 다양성을 높이기 위해 저자는 기존의 방식과 다른 recursive way를 적용했다고 한다. policy \({\mathcal P}\) 은 여러 개의 operation로 구성되어 있다. 위 그림 1에서는 4개의 \({\mathcal O}\) 로 구성되어 있으며, 논문에서는 이 개수를 N이라고 한다. 논문에서는 여러 증강 파라미터에서의 원활한 비교를 위해, 랜덤으로 N개의 operation 중에서 N \({*}\) 를 랜덤으로 샘플링한다. 그림 1에서는 2로 설정되어 있다.</p>

<p><img src="/assets/img/TAA_eqution1-1.png" alt="" width="500" height="500" class="align-center" /></p>

<p>이렇게 2개를 뽑는 이유는, 물론 여러 표현이 담긴 증강을 문장을 생성하는 것 뿐만 아니라, \({p}\) 에 의해 증강이 안될 경우를 고려한 것이라고 생각한다. 2개의 증강 operation을 순차적으로 적용시켜 최종적으로 문장을 증강하는 것이다. 저자는 이러한 방식을 통해 하나의 policy가 양질의 문장을 증강할 수 있음을 말하고 있다. 또한 policy는 \({N \times 3}\) 에 의해 결정되기 떄문에 search space explosion 문제를 유발하지 않는다고 한다. <strong>(아마도 과도한 증강이나, 시간 문제를 막으려는 것으로 보인다.)</strong> 저자는 이러한 방식이 이전의 naive grid search, cumbersome manual tuning를 대신에 자동적으로 최적의 파라미터를 찾는데 도움이 된다고 한다</p>

<h2 id="22-learnable-data-augmentation-policy">2.2 Learnable Data Augmentation Policy</h2>
<p>이번장에서는 전통적인 모델의 학습 방식에서 최적의 증강 policy를 찾는지, 자세하게 설명한다. 매우 작고, 불균형한 데이터 셋 \({\mathcal D_{train}}\) 이 존재할 때, 모델 \({\mathcal f}\) 는 large generalization error를 유발한다. 따라서 데이터 증강은 모델이 데이터 셋의 패턴 학습 성능과 일반화 능력을 높이도록 implicit regularizer로 통합된다. <strong>(아직 이 문장에 대한 의미를 자세히 모르지만, 적어두어야 할 것 같아서 적었음.)</strong></p>

<p>원본 학습 데이터 셋과 policy \({\mathcal P}\) 로 증강된 문장 데이터셋을 통합한 \({\mathcal D_{aug}(\mathcal P)}\) 가 있다고 하자. 그러면 모델을 cross-entropy와 같이 loss를 아래와 같이 구하게 된다.</p>

<p><img src="/assets/img/TAA_eqution2.png" alt="" width="500" height="500" class="align-center" /></p>

<p>이전의 방식은, 증강 파라미터를 학습 전에 설정해준 다음, validation 셋을 테스트하면서 미세 조정하게 된다. 이는 하이퍼 파라미터 튜닝 패러다음과 유사하다고 한다. 이러한 이유로 저자는 AutoML에서의 Combined Algorithm Selection and Hyper-parameter(CASH) 최적화 문제를 통해, 증강 파라미터 선택에서의 최적화 방식을 비교해보았다.</p>

<p><img src="/assets/img/TAA_eqution3.png" alt="" width="500" height="500" class="align-center" /></p>

<p>위 사진을 보면, 원본+증강 학습 데이터 셋을 학습하고 나서 원본 검증 셋과 비교하여 loss를 구한다. 이러한 방식은 일반화 능력과 양질의 문장을 생성할 수 있다고 한다. <strong>(아마도 검증 데이터 셋과 비슷하게 증강하도록 파라미터를 학습하는 것이 아닌가 생각된다. 그러면 학습 데이터가 검증 데이터 셋과 유사해질 가능성이 있는데…)</strong></p>

<p><img src="/assets/img/TAA_eqution4.png" alt="" width="500" height="500" class="align-center" /></p>

<p>따라서 최종적으로, 모델이 val loss를 최소화하면서 최적의 증강 파라미터(policy)를 찾도록 위와 같이 최적화 공식을 세울 수 있다. \({\mathbb F, \mathbb P}\) 를 통해, 여러 개의 모델을 동시적으로 학습시켜 가장 좋은 성능을 보이는 모델을 택하는 것으로 이해하였다.</p>

<h2 id="23-augmentation-policy-optimization">2.3 Augmentation Policy Optimization</h2>
<p>다른 최적화 방식과 동일하게, 본 모델도 val loss에 의존한다. 그러나 역전파 과정와 같은 그래디언트 방식의 학습 방법에서 val loss는 과적합 판단 요소에 쓰일 뿐, 최적의 증강 파라미터를 찾을 수 없다. 이러한 문제를 해결하기 위해 저자는 AutoML에서 널리 사용되는 베이지안 옵티마이져 형태의 Sequential Model-based Global Opti- mization (SMBO)를 사용하였다. 저자의 최적화 과정은 사진 3과 같다.</p>

<p><img src="/assets/img/TAA_figure3.png" alt="" width="500" height="700" class="align-center" /></p>

<p>2장에서 논의할 분포의 역할들 중 하나는 한정된 수의 관찰 집합 $x_{1},…,x_{N}$이 주어졌을 때 확률 변수 $x$의 확률 분포 $p(x)$를 모델링하는 것이다. 딥러닝 모델에서 하이퍼 파라미터를 최적화하는 방식은, validation loss에 의존한다. 이는 back-propagation와 같은 gradient-based methods로는 풀수 없는 문제이다. 따라서 저자는 <strong>(Sequential Model-based Global Optimization (SMBO))</strong> 를 제안한다. 이는 AutoML에서 널리 쓰이는 베이지만 최적화 방식이다. 전체적인 최적화 과정은 그림 3과 같다. SMBO 옵티마이져는 surrogate로써 목표 함수의 확률 모델을 생성하며, 가장 효과가 좋은 policy를 찾아내고 이를 목적 함수에 평가한다. 실제로 저자는 objective loss와 policy간의 함수를 모델링 하기 위해, surrogate로 agent M인 <strong>(Treestructured Parzen Estimator (TPE))</strong> 를 사용한다.</p>

<p><img src="/assets/img/TAA_Algorithm1.png" alt="" width="500" height="700" class="align-center" /></p>

<p>전반적인 프로세스는 Algorithm와 같다. 각 iteration마다 SMBO 옵티마이져는 증강 policy를 샘플링하고, 이를 기반으로 모델 \({\mathcal f_{i}}\) 를 학습한다. policy \({\mathcal P_{i}}\)의 loss \({\mathcal J_{i}}\)는 수식 7에 의해 구해지고, surrogate 모델을 업데이트 하고자 observation histort \({\mathcal H}\)에 합쳐진다. TPE의 구체적인 업데이트 과정은 Appendix B에 언급되어있고, 다음 스텝에서 쓸만한 policy를 샘플링하기 위해 Expected Improvement (EI) criterion를 사용한다.</p>

<p><img src="/assets/img/TAA_eqution5.png" alt="" width="500" height="500" class="align-center" /></p>

<p>여기서 \({\dagger}\)는 threshold값으로, surrogate 모델과 \({\mathcal H}\)에 의해 계산된 값이다. 수식 8은 loss가 threshold값보다 작게 나왔을 때, 해당 loss의 policy로 사용하겠다는 뜻이다. 해당 프로세스의 목표는 가장 성능이 좋은 policy의 loss를 최소화 하는 것이고, 다음 iteration에 사용될 policy의 개수를 증가시켜서 성능 향상을 유도하는 것이다. 이러한 과정은 모두 Algorithm1에 명시되어 있다.</p>

<h2 id="3-experiments">3. Experiments</h2>
<h2 id="31-benchmark-datasets">3.1 Benchmark Datasets</h2>
<p>저자가 성능 측정에 사용된 데이터 셋은 table 8과 같다.</p>

<p><img src="/assets/img/TAA_table8.png" alt="" width="700" height="700" class="align-center" /></p>

<h2 id="32-baselines">3.2 Baselines</h2>
<p>저자는 TAA 기술과 5가지의 증강 기술을 비교하고자 하였다.
<img src="/assets/img/TAA_table1.png" alt="" width="500" height="500" class="align-center" /></p>

<p><strong>1. Back Translation (BT)</strong> : BT 모델로는 WMT’19 EnglishGerman translation model를 사용하였으며, (Xie et al., 2020; Edunov et al., 2018)가 추천하는 대로 디코딩 과정에서 랜덤 샘플링을 진행하였다. 또한 다양한 paraphrases를 생성하기 위해 temperature를 0.8로 설정하였다.</p>

<p><strong>2. ontextual Word Substitute (CWS)</strong> : CWS는 확률적으로 문장에서 일부 단어를 mask하고 label-conditional language model(LM)를 사용하여 mask된 단어를 예측한다. mask 확률은 0.15로 설정하였다.</p>

<p><strong>3. Easy Data Augmentation (EDA)</strong> : EDA는 Synonym Replacement, Random Insertion, Random Swap, Random Deletion의 4가지 방법으로 증강을 진행하여, 수정될 단어의 비율을 0.05로 설정하였다.</p>

<p><strong>4. Learning Data Manipulation (LDM)</strong> : LDM은 learnable한 데이터 증강 방식이지만, CWS기반의 단 한가지 opeation만을 사용한다. 이 방식은 TAA와 유사하게, 학습 과정에서 최적의 파라미터를 찾는 방식이다.</p>

<h2 id="33-experimental-setting">3.3 Experimental Setting</h2>
<p>저자는 uncased 버전인 BERT-base 모델을 사용하였다. Adam과 learning rate 4e-5의 linear warmup scheduler를 사용하였고 TREC에는 20 epoch, 다른 데이터 셋에는 10 epoch를 사용하였으며 val loss를 기준으로 최적의 모델을 선정하였다. val loss는 앞서 말했듯이 최적의 policy를 찾는 기준이며, 동일한 사이즈의 2개의 validation 셋을 사용하였다. 하나는 모델 성능 체크, 나머지 하나는 샘플링된 policy 평가용이다. <strong>(저자는 2가지 시나리오 환경에서 성능을 측정하고자 하였다.)</strong></p>

<p><strong>Low-resource Regime</strong> : 적은 데이터 환경에서의 성능을 측정하기 위함이다. 원본 데이터와 동일한 분포를 얻기 위해, 저자는 \((Stratified ShuffleSplit)\) 를 통해 train과 val set으로 나누었다. IMDB, SST-5, TREC, YELP-2, YELP-5 데이터 셋들은 각각 80,200,120,80,200의 라벨 데이터를 가지고 있으며, 이는 성능 향상에 상당한 제약이 있다. val 데이터 셋은 60, 150, 60, 60, 150이다. 적은 데이터 환경에서 저자는 \({n_{aug}}\) 를 사용하며, \({n_{aug}}\) = 16인 경우, 각 주어진 샘플마다 16개의 샘플을 합성한다.</p>

<p><strong>Class-imbalanced Regime.</strong> : 증강이 필요한 환경은 대표적으로 “적은 데이터 환경”와 “불균형 데이터 환경”이 있다. 이러한 환경을 구성하기 위해 긍정, 부정 환경에서 부정 데이터셋은 1000개의 샘플, 긍정은 20/50/100으로 설정하고 증강을 50/20/10배로 하여 균형을 맞추었다. 또한 저자는 추가로 <strong>Over-Sampling (OS)</strong> 시나리오를 추가하였는데, 긍정 데이터 셋에 추가적(?)으로 증강 과정을 진행한 것으로 판단된다.
Test 데이터 셋은 어떠한 감소도 없이 균형하게 설정하였고, 평가 과정에서 공평하게 사용되었다. Policy \({N}\)은 8, N 내부의 증강 operation \({N^{*}}\)은 2로 설정하였다. 또한 공평한 비교를 위해, 데이터 sub-sampling을 위한 랜덤 시드로 3을 사용하였고, 각 시드 별로 5번 이하로 구현(run)하였다.</p>

<h2 id="33-experimental-setting-1">3.3 Experimental Setting</h2>
<p><img src="/assets/img/TAA_table2.png" alt="" width="900" height="900" class="align-center" /></p>

<p><strong>Low-resource Regime</strong> : Table 2는 실험 결과(acc)를 보여준다. TAA는 모든 데이터 셋에서, 5가지의 다른 증강 기법보다 높은 성능을 달성하였다. 그림 1에서 볼 수 있듯이, 증강 파라미터의 크기는 성능과 매우 관련이 깊다. EDA, CWS와 같이 heuristic 기반 기법은 수동적으로 파라미터를 선정하기 때문에 차선책(sub-optimum)에 그칠 수 있다. 반대로 TAA기법은 learnable 하고 compositional 하기 때문에, 높은 성능을 달성할 수 있었다. 평균적으로 8.8%의 성능 향상이 있었으며, 학습 시간에 많은 시간이 소요되지도 않는다. Algorithm 1에서 보이는 iteration \({T}\)를 200으로 설정하였을 때, 8개의 NVIDIA Tesla P40 GPU를 사용하여 4시간이 소요되었다.</p>

<p><img src="/assets/img/TAA_table3.png" alt="" width="900" height="900" class="align-center" /></p>

<p><strong>Class-imbalanced Regime.</strong> : Table 3는 실험 결과(acc)를 보여준다. over-sampling method는 과적합의 문제를 일부 해소해주지만, 증강 baseline들에 비하면 떨어지는 것이 사실이다. 그러나 TAA는 다른 알고리즘보다 9.7%의 성능 향상을 보였다.</p>

<p>최종적으로 multiple learnable TAA는 2가지의 데이터 환경에서 비교한 5가지의 증강 기법을 능가한다.</p>

<h2 id="4-analysis">4. Analysis</h2>
<p>저자는 4가지(transferability, scalability, magnification, structure)) 양상에서 학습된 policy들을 분석하였다. 추가로 다양성과 문장의 의미 보존, 추가 연구 사례를 평가한다. TAA에 의해 생성된 최종 policy는 Table9와 같다.</p>

<p><img src="/assets/img/TAA_table9.png" alt="" width="900" height="900" class="align-center" /></p>

<h2 id="41-transferability">4.1 Transferability</h2>
<p>저자는 하나의 데이터셋에서 찾은 최적의 policy를 다른 데이터셋에 적용하였을 때의 성능을 비교하였다. Figure 4에서 볼 수 있듯이, 다른 데이터셋에 policy를 적용시에 1.1%의 성능 하락이 일어났지만, 증강을 진행하지 않았을 때와 비교할 땐 test에서 7.7%의 성능 향상을 이뤘다고 한다.<strong>(…?)</strong></p>

<p><img src="/assets/img/TAA_figure4.png" alt="" width="500" height="500" class="align-center" /></p>

<h2 id="42-scalability">4.2 Scalability</h2>
<p>저자는 TAA policy가 학습 데이터를 증가시키는지 조사하기 위해, 2가지 데이터 환경에서 실험을 진행했었다. 그리고 추가적으로 2가지 실험 환경이 아닌, 원본 데이터 전부에 증강 작업을 실시하여 성능을 비교하였다. 실험 환경에서의 유일한 차이점은 학습 데이터의 수이며, 테스트 데이터의 수는 동일하게 설정하였었다. Table 4는 학습 결과이며, 데이터의 제한 사항에서만 효과적인 것이 아닌, 원본 데이터에서도 효과적임을 알 수 있다.</p>

<p><img src="/assets/img/TAA_table4.png" alt="" width="500" height="500" class="align-center" /></p>

<h2 id="43-impact-of-augment-magnification">4.3 Impact of Augment Magnification</h2>
<p><img src="/assets/img/TAA_figure5.png" alt="" width="500" height="500" class="align-center" /></p>

<p>Figure 5는 각 증강 method 마다 \({n_{aug}}\)의 크기에 따른 성능 비교이다. CWS, LDM은 \({n_{aug}}\) = 4에서 가장 높은 성능을 보였으나 \({n_{aug}}\)의 크기가 커질수록 성능이 감소함을 볼 수 있다. TAA의 증강 방식뿐만 아니라, 비교한 증강 방법들은 <strong>(수정)</strong> 기반의 증강 방식이다. 따라서 \({n_{aug}}\) 의 수가 증가하더라도, 새로운 문장 표현 생성이 어렵기 때문에 더 높은 성능 향상을 이루어 낼 순 없었다. EDA 방식은 unlearnable하고, 사람이 직접 파라미터를 설정하기 때문에 TREC의 경우 상대적으로 낮은 성능을 보인다. 그러나 TAA는 모든 \({n_{aug}}\)에서 높은 성능 향상을 보였다.</p>

<h2 id="44-impact-of-the-policy-structure">4.4 Impact of the Policy Structure</h2>
<p><img src="/assets/img/TAA_figure6.png" alt="" width="500" height="500" class="align-center" /></p>

<p>저자는 최적의 policy \({N}\)과 N 내부의 증강 operation \({N^{*}}\)를 찾기 위한 추가 실험을 진행하였다. Figure 6은 실험 결과이다. \({N^{*}}\)이 증가하면 약간의 성능 하락이 발생하지만, \({N}\)이 증가하면 성능은 상승하는 양상을 보인다. 이러한 결과는 다양한 표현의 문장을 생성하도록 하는 policy가 많으면 많을수록 좋지만, Figure 5처럼 한계점은 존재한다. 다른 설정, 환경에서의 실험은 Appendix E에 언급되어 있다.</p>

<h2 id="45-diversity-of-augmented-data">4.5 Diversity of Augmented Data</h2>
<p><img src="/assets/img/TAA_table5.png" alt="" width="500" height="500" class="align-center" /></p>

<p>저자는 증강된 문장의 다양성을 측정하기 위해 Dist-2(Li et al., 2016)를 사용하였다. 1에 가까울 수록 다양성이 높음을 의미하고, 0에 가까울수록 다양성이 없음을 의미한다. Tabke 5에서 볼 수 있듯이, TAA가 다양성 높은 문장을 생성해냄을 알 수 있다. 수정 방식의 증강 방법이 생성 방식의 증강 방법인 BT와 거의 유사한 성능을 보이는데, 이는 데이터 셋에 맞게 다양한 증강 기법와 변환된 단어의 수를 학습 과정에서 최적의 값을 찾아 파인튜닝하기 때문이다.</p>

<h2 id="46-semantics-preservation-of-augmented-data">4.6 Semantics Preservation of Augmented Data</h2>

<p>데이터 증강의 중요한 부분은 원본 문장의 의도를 훼손하기 않고 보존해야 하는 것이다. 그래서 저자는 문장 임베딩간의 코사인 유사도를 통해 의미 보존을 얼마나 하는지 측정하였다. 비교 모델로는 문장 유사도 task에서 SOTA를 달성한 Sentence-BERT를 사용하였다. 의미 보존 점수 \({SP(x, x_{aug})}\)는 식 6과 같으며, 결과는 Table 6에 명시되어 있다.</p>

<p><img src="/assets/img/TAA_eqution6.png" alt="" width="300" height="300" class="align-center" /></p>

<p><img src="/assets/img/TAA_table6.png" alt="" width="700" height="700" class="align-center" /></p>

<p>TAA는 문장의 다양성 뿐만 아니라, 문장의 원래 의도를 보존함에 있어서도 좋은 성능을 보임을 알 수 있다.</p>

<h2 id="47-case-study">4.7 Case Study</h2>
<p>Figure 7은 TAA를 통해, 매 iteration \({T}\)가 처음에는 이상하게 증강되어지는 모습을 보이지만, 증가할수록 증강되는 문장이 점차 완성도 있음을 확인할 수 있다.</p>

<p><img src="/assets/img/TAA_figure7.png" alt="" width="700" height="700" class="align-center" /></p>

<h2 id="5-related-work">5 Related Work</h2>
<p>이전 증강 방식은 크게 “생성”, “수정”의 방식으로 구현되었다. “생성” 방식은 BT, LM, GPT-2, BART 등을 통해 수행되었고, “수정” 방식은 EDA, TF-IDF 등으로 수행되었다. 최근 증강 알고리즘은 증강 파라미터를 policy를 통해 자동으로 구하는 방식이 쓰이고 있다. 그러나 저자는 이들의 방식이 본인들의 모델링 방식하고는 다름을 말한다. 구체적으로 저자는 <strong>Reinforcement Learning (RL)</strong> 라는 워딩을 사용하였다. 다른 연구에서 과도한 policy로 인해 학습 시간을 줄이고자 population-based algorithm을 사용한것과 교차 검증 알고리즘을 통해 validation 데이터를 사용하여 증강한 아이디어에 착안한 것으로 보인다. 이후에는 본인들의 연구에 대한 장점이 수록되어 있지만 생략하였다.</p>

<h2 id="ignore">ignore</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">total</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">sys</span><span class="p">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">print</span> <span class="s">'sum ='</span><span class="p">,</span> <span class="n">total</span>
<span class="k">except</span> <span class="nb">ValueError</span><span class="p">:</span>
    <span class="k">print</span> <span class="s">'Please supply integer arguments'</span>
</code></pre></div></div>

:ET